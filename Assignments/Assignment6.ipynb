{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 6\n",
    "\n",
    "Import the necessary libraries\n",
    "\n"
   ],
   "id": "68fc9490ef0caa28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:15.757717Z",
     "start_time": "2025-04-26T09:15:15.754941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow import keras\n",
    "from keras import ops\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow as tf\n",
    "import random"
   ],
   "id": "e96a1ec4af383477",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "the the Finnish-Enlish translation dataset"
   ],
   "id": "28c023a709dc0ab2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:15.897605Z",
     "start_time": "2025-04-26T09:15:15.806235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_file = \"../Datasets/fin.txt\"\n",
    "\n",
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, finnish, rest = line.split(\"\\t\")\n",
    "    finnish = \"[start] \" + finnish + \" [end]\"\n",
    "    text_pairs.append((finnish, english))"
   ],
   "id": "bc557dccbd27c750",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the dataset and split it into training, validation, and test sets. The portions of the dataset are 70% training, 15% validation, and 15% test."
   ],
   "id": "5a09bf0f0c091174"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:15.950016Z",
     "start_time": "2025-04-26T09:15:15.905689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ],
   "id": "3bc4d5973e303416",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Strip the punctuation from the text and remove \"[\" and \"]\" from the punctuation list. Define a custom standardization function that converts the text to lowercase and removes the punctuation.",
   "id": "ce64399b6a28d39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:15.961363Z",
     "start_time": "2025-04-26T09:15:15.958405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
   ],
   "id": "fca581b703e5b6e9",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "\n",
    "Define the vocabulary size as 15 000 and sequence length as 40. Create two `TextVectorization` layers, one for the source language (Finnish) and one for the target language (English). The `TextVectorization` layer will be used to convert the text into integer sequences. The `adapt` method is called on both layers to fit them to the training data."
   ],
   "id": "956c9f49e5029b97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:16.230255Z",
     "start_time": "2025-04-26T09:15:16.009272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 40\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_finnish_texts = [pair[0] for pair in train_pairs]\n",
    "train_english_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_finnish_texts)\n",
    "target_vectorization.adapt(train_english_texts)"
   ],
   "id": "103b61875168eb28",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creation Of The Dataset\n",
    "\n",
    "The batch size is defined to be 64. Define a function `format_dataset` that takes the Finnish and English text as input and returns a dictionary with the Finnish text and the English text shifted by one position. The `make_dataset` function creates a TensorFlow dataset from the pairs of Finnish and English text, shuffles it, and caches it for performance."
   ],
   "id": "2e1285d1cec1ecfe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(fin, eng):\n",
    "    fin = source_vectorization(fin)\n",
    "    eng = target_vectorization(eng)\n",
    "    return ({\n",
    "                \"finnish\": fin,\n",
    "                \"english\": eng[:, :-1],\n",
    "            }, eng[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    fin_texts, eng_texts = zip(*pairs)\n",
    "    fin_texts = list(fin_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((fin_texts, eng_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ],
   "id": "9de993deee494aa7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Print the shapes of the inputs and target in the dataset.",
   "id": "cb937197f5504c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:16.773413Z",
     "start_time": "2025-04-26T09:15:16.676554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['finnish'].shape: {inputs['finnish'].shape}\")\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ],
   "id": "85823abd14c5b34a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['finnish'].shape: (64, 40)\n",
      "inputs['english'].shape: (64, 40)\n",
      "targets.shape: (64, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 12:15:16.769401: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional Embedding Layer\n",
    "The `PositionalEmbedding` layer is defined to add positional information to the token embeddings. The `call` method computes the token and position embeddings and returns their sum. The `compute_mask` method creates a mask for the input sequences so that the padding tokens are ignored during training. The `get_config` method returns the configuration of the layer."
   ],
   "id": "162729d30352c8c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:16.786109Z",
     "start_time": "2025-04-26T09:15:16.782026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return ops.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "        \"output_dim\": self.output_dim,\n",
    "        \"sequence_length\": self.sequence_length,\n",
    "        \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ],
   "id": "8b33cd5a8fbbfbcb",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Encoder and Decoder Layers\n",
    "\n",
    "The `TransformerEncoder` is defined to create the encoder part of the transformer model. It uses multi-head attention and a feed-forward network. The `call` method computes the attention output and applies layer normalization."
   ],
   "id": "eb558d1f3f5e34ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:16.842181Z",
     "start_time": "2025-04-26T09:15:16.838027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ],
   "id": "fdc8eae870fffb29",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`TransformerDecoder` is defined to create the decoder part of the transformer model. It uses multi-head attention and a feed-forward network. The `call` method computes the attention output and applies layer normalization. The `get_causal_attention_mask` method creates a causal attention mask for the decoder.",
   "id": "7a9fb771db703890"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:16.914744Z",
     "start_time": "2025-04-26T09:15:16.909641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim)])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        padding_mask = None\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(query=attention_output_1, value=encoder_outputs, key=encoder_outputs, attention_mask=padding_mask)\n",
    "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n"
   ],
   "id": "1cbdd335a19d2e0e",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating The Transformer Model\n",
    "\n",
    "The transformer model is created by defining the encoder and decoder inputs. The `PositionalEmbedding` layer is applied to both the encoder and decoder inputs. The `TransformerEncoder` and `TransformerDecoder` layers are applied to the inputs. Finally, dropout of 50% and a dense layer with softmax activation is added to the decoder outputs."
   ],
   "id": "c2c23a044427f5b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:15:17.253829Z",
     "start_time": "2025-04-26T09:15:16.958107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"finnish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ],
   "id": "a78db2c01c079a63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefu/anaconda3/envs/keras/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'transformer_encoder_1' (of type TransformerEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compile And Train The Model\n",
    "\n",
    "The model is compiled with the RMSprop optimizer and sparse categorical crossentropy loss. The model is trained for 30 epochs with a batch size of 64. The validation data is used to evaluate the model during training."
   ],
   "id": "44b56669a6391f88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:35:01.108297Z",
     "start_time": "2025-04-26T09:15:17.262664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer.compile(\n",
    " optimizer=\"rmsprop\",\n",
    " loss=\"sparse_categorical_crossentropy\",\n",
    " metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ],
   "id": "771e18a6ff0eabb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745658918.953806    1618 service.cc:152] XLA service 0x2ca9fe40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745658918.953951    1618 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 SUPER, Compute Capability 8.9\n",
      "2025-04-26 12:15:19.090850: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-04-26 12:15:19.212824: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-04-26 12:15:21.224961: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 432 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:21.278679: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 452 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:21.356573: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 556 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:21.426728: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:21.762021: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_77', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:21.930496: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 2620 bytes spill stores, 4484 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.002312: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 488 bytes spill stores, 488 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.008759: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_77', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.012786: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 820 bytes spill stores, 820 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.368095: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.489379: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.587048: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 436 bytes spill stores, 436 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.729929: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 452 bytes spill stores, 356 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.887977: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14_0', 1604 bytes spill stores, 1552 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:22.906078: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.008837: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 3148 bytes spill stores, 3124 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.009763: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.034720: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 532 bytes spill stores, 532 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.231823: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 540 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.274677: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.418531: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14_0', 136 bytes spill stores, 136 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.161017: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.246372: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 524 bytes spill stores, 524 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.254252: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.326883: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.432148: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 432 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.536605: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 4132 bytes spill stores, 4124 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:23.911684: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.021827: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 2636 bytes spill stores, 4500 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.066625: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.070565: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.173633: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.174838: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.301053: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.353734: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 556 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.369165: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.541529: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.693158: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 504 bytes spill stores, 504 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:24.881646: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.027772: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.052566: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.138584: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.567319: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.611698: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_88', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.686436: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.702221: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.758454: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 540 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:25.957622: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 4132 bytes spill stores, 4124 bytes spill loads\n",
      "\n",
      "I0000 00:00:1745658931.840450    1618 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m380/791\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m25s\u001B[0m 61ms/step - accuracy: 0.8588 - loss: 6.0763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 12:15:49.667080: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-04-26 12:15:51.287439: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 432 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:51.390571: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 428 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:51.659088: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 108 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.126913: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 3348 bytes spill stores, 5136 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.252025: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.252723: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 432 bytes spill stores, 340 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.418972: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_77', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.429191: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.477238: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 864 bytes spill stores, 864 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.748849: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:52.798861: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.219324: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14_0', 1464 bytes spill stores, 1156 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.238248: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 112 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.265082: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14_0', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.409079: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_85', 436 bytes spill stores, 436 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.439151: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_73', 540 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.511394: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 420 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "2025-04-26 12:16:00.171598: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-04-26 12:16:00.181334: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_76', 3140 bytes spill stores, 4936 bytes spill loads\n",
      "\n",
      "2025-04-26 12:16:00.221859: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.344342: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 420 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.579159: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_84', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.714859: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.763188: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 532 bytes spill stores, 532 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.763294: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.784351: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 540 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.786304: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:53.788254: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.021650: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.244540: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.402168: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.523579: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_14', 3148 bytes spill stores, 3124 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.532696: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_88', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.687021: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 524 bytes spill stores, 524 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:54.793799: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41_0', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.034295: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.038492: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 444 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.042211: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51_0', 1288 bytes spill stores, 1288 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.127157: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 504 bytes spill stores, 504 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.180170: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 540 bytes spill stores, 420 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.299251: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.662026: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.687505: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.737059: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 4600 bytes spill stores, 4520 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.755882: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.815486: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3715', 476 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.921463: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_41', 4132 bytes spill stores, 4124 bytes spill loads\n",
      "\n",
      "2025-04-26 12:15:55.974295: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_51', 4132 bytes spill stores, 4124 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m661/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m9s\u001B[0m 73ms/step - accuracy: 0.8535 - loss: 5.7258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 12:16:18.233414: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2025-04-26 12:16:20.404425: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:39] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 63ms/step - accuracy: 0.8489 - loss: 5.6177 - val_accuracy: 0.6241 - val_loss: 4.5230\n",
      "Epoch 2/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 46ms/step - accuracy: 0.7841 - loss: 4.4282 - val_accuracy: 0.8215 - val_loss: 3.7718\n",
      "Epoch 3/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - accuracy: 0.7467 - loss: 3.6896 - val_accuracy: 0.7917 - val_loss: 3.0303\n",
      "Epoch 4/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - accuracy: 0.7614 - loss: 2.9850 - val_accuracy: 0.8322 - val_loss: 2.7407\n",
      "Epoch 5/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - accuracy: 0.7689 - loss: 2.5443 - val_accuracy: 0.7849 - val_loss: 2.5761\n",
      "Epoch 6/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - accuracy: 0.7864 - loss: 2.2523 - val_accuracy: 0.8555 - val_loss: 2.5222\n",
      "Epoch 7/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - accuracy: 0.8053 - loss: 2.0408 - val_accuracy: 0.8202 - val_loss: 2.5141\n",
      "Epoch 8/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8110 - loss: 1.8943 - val_accuracy: 0.8298 - val_loss: 2.5312\n",
      "Epoch 9/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 48ms/step - accuracy: 0.7943 - loss: 1.7899 - val_accuracy: 0.8161 - val_loss: 2.5267\n",
      "Epoch 10/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - accuracy: 0.7977 - loss: 1.7003 - val_accuracy: 0.8472 - val_loss: 2.5426\n",
      "Epoch 11/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8166 - loss: 1.6337 - val_accuracy: 0.8022 - val_loss: 2.5672\n",
      "Epoch 12/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8217 - loss: 1.5818 - val_accuracy: 0.8818 - val_loss: 2.6955\n",
      "Epoch 13/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8189 - loss: 1.5274 - val_accuracy: 0.7469 - val_loss: 2.5899\n",
      "Epoch 14/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8133 - loss: 1.4872 - val_accuracy: 0.8040 - val_loss: 2.6485\n",
      "Epoch 15/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 50ms/step - accuracy: 0.8035 - loss: 1.4517 - val_accuracy: 0.7942 - val_loss: 2.6253\n",
      "Epoch 16/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8264 - loss: 1.4128 - val_accuracy: 0.7267 - val_loss: 2.6777\n",
      "Epoch 17/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8190 - loss: 1.3779 - val_accuracy: 0.7238 - val_loss: 2.6969\n",
      "Epoch 18/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8287 - loss: 1.3564 - val_accuracy: 0.8130 - val_loss: 2.7343\n",
      "Epoch 19/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8385 - loss: 1.3254 - val_accuracy: 0.7640 - val_loss: 2.7618\n",
      "Epoch 20/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 50ms/step - accuracy: 0.8419 - loss: 1.3034 - val_accuracy: 0.7213 - val_loss: 2.7545\n",
      "Epoch 21/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.7766 - loss: 1.2850 - val_accuracy: 0.7949 - val_loss: 2.7651\n",
      "Epoch 22/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8415 - loss: 1.2563 - val_accuracy: 0.7308 - val_loss: 2.7877\n",
      "Epoch 23/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8064 - loss: 1.2378 - val_accuracy: 0.8016 - val_loss: 2.8296\n",
      "Epoch 24/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8408 - loss: 1.2209 - val_accuracy: 0.8658 - val_loss: 2.8525\n",
      "Epoch 25/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - accuracy: 0.8723 - loss: 1.2046 - val_accuracy: 0.8721 - val_loss: 2.8732\n",
      "Epoch 26/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8699 - loss: 1.1874 - val_accuracy: 0.8976 - val_loss: 2.8954\n",
      "Epoch 27/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 49ms/step - accuracy: 0.8747 - loss: 1.1754 - val_accuracy: 0.8355 - val_loss: 2.9129\n",
      "Epoch 28/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 50ms/step - accuracy: 0.8651 - loss: 1.1595 - val_accuracy: 0.9031 - val_loss: 2.9341\n",
      "Epoch 29/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - accuracy: 0.8827 - loss: 1.1471 - val_accuracy: 0.8667 - val_loss: 2.9671\n",
      "Epoch 30/30\n",
      "\u001B[1m791/791\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 49ms/step - accuracy: 0.8776 - loss: 1.1356 - val_accuracy: 0.8396 - val_loss: 2.9318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f0f78316e10>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluate The Model\n",
    "\n",
    "The model is evaluated on the test set. The `decode_sequence` function is defined to decode the input sentence and generate the translated output. The function uses the trained transformer model to predict the output sequence. Few examples of translations are printed that show that the model is able to translate some sentences accurately, but some translations are short and not accurate."
   ],
   "id": "c6d48edebed217cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T09:39:43.521900Z",
     "start_time": "2025-04-26T09:39:40.319265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eng_vocab = target_vectorization.get_vocabulary()\n",
    "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence]\n",
    "        )[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence]\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = eng_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_fin_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(10):\n",
    "    print(\"Translation \" + str(i + 1) + \":\")\n",
    "    input_sentence = random.choice(test_fin_texts)\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))\n",
    "    print(\"\")"
   ],
   "id": "8862d36ad2aeeaf1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 1:\n",
      "[start] Tomi hämmentyi. [end]\n",
      "[start] to my dog                 \n",
      "\n",
      "Translation 2:\n",
      "[start] Voisin lähteä pizzalle nyt heti. [end]\n",
      "[start] can i leave right away  now     away   to from    \n",
      "\n",
      "Translation 3:\n",
      "[start] Minä alan väsyä. [end]\n",
      "[start] are you starting                 \n",
      "\n",
      "Translation 4:\n",
      "[start] Jatketaan peliä lounaan jälkeen. [end]\n",
      "[start] the game after lunch                \n",
      "\n",
      "Translation 5:\n",
      "[start] Minkä takia ostit tämän kalliin sanakirjan? [end]\n",
      "[start] why did you buy this expensive dictionary for            \n",
      "\n"
     ]
    }
   ],
   "execution_count": 101
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
